{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch_geometric as tg\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define paths\n",
    "Load timeseries and find id of subject with odd length timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(\"..\", \"data\", \"cobre_difumo512\")\n",
    "ts_path = os.path.join(data_path, \"difumo\", \"timeseries\")\n",
    "conn_path = os.path.join(data_path, \"difumo\", \"connectomes\")\n",
    "pheno_path = os.path.join(data_path, \"difumo\", \"phenotypic_data.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad sub ID: 40075\n"
     ]
    }
   ],
   "source": [
    "timeseries = [np.load(os.path.join(ts_path, p)) for p in os.listdir(ts_path)]\n",
    "ids = [int(p.split('_')[1]) for p in os.listdir(ts_path)]\n",
    "\n",
    "# One subject has different length timeseries, ignore them for now\n",
    "not_150 = np.array([t.shape[0]!=150 for t in timeseries])\n",
    "print('Bad sub ID: {}'.format(np.array(ids)[not_150][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Graph\n",
    "- Load connectomes\n",
    "- Get avg connectome\n",
    "- Get 8 knn graph from avg connectome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_undirected(mat):\n",
    "    \"\"\"Takes an input adjacency matrix and makes it undirected (symmetric).\"\"\"\n",
    "    m = mat.copy()\n",
    "    mask = mat != mat.transpose()\n",
    "    vals = mat[mask] + mat.transpose()[mask]\n",
    "    m[mask] = vals\n",
    "    return m\n",
    "\n",
    "def knn_graph(mat,k=8):\n",
    "    \"\"\"Takes an input matrix and returns a k-Nearest Neighbour weighted adjacency matrix.\"\"\"\n",
    "    is_undirected = (mat == mat.T).all()\n",
    "    m = np.abs(mat.copy())\n",
    "    np.fill_diagonal(m,0)\n",
    "    slices = []\n",
    "    for i in range(m.shape[0]):\n",
    "        s = m[:,i]\n",
    "        not_neighbours = s.argsort()[:-k]\n",
    "        s[not_neighbours] = 0\n",
    "        slices.append(s)\n",
    "    if is_undirected:\n",
    "        return np.array(slices)\n",
    "    else:\n",
    "        return make_undirected(np.array(slices))\n",
    "    \n",
    "def make_group_graph(connectomes,k=8):\n",
    "    # Group average connectome\n",
    "    avg_conn = np.array(connectomes).mean(axis=0)\n",
    "\n",
    "    # Undirected 8 k-NN graph as matrix\n",
    "    avg_conn8 = knn_graph(avg_conn,k=k)\n",
    "\n",
    "    # Format matrix into graph for torch_geometric\n",
    "    graph = nx.convert_matrix.from_numpy_array(avg_conn8)\n",
    "    return tg.utils.from_networkx(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get train/test/validation data\n",
    "- Load timeseries and ids\n",
    "- Split timeseries of 150 volumes into time windows\n",
    "- Split data into train/test/validation\n",
    "  - All data from a given subject goes in the same bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_timeseries(ts,n_timepoints=50):\n",
    "    \"\"\"Takes an input timeseries and splits it into time windows of specified length. Need to choose a number that splits evenly.\"\"\"\n",
    "    if ts.shape[0] % n_timepoints != 0:\n",
    "        raise ValueError('Yikes choose a divisor for now')\n",
    "    else:\n",
    "        n_splits = ts.shape[0] / n_timepoints\n",
    "        return np.split(ts,n_splits)\n",
    "\n",
    "def split_ts_labels(timeseries,labels,n_timepoints=50):\n",
    "    \"\"\"\n",
    "    timeseries: list of timeseries\n",
    "    labels: list of lists (of accompanying labels)\n",
    "    n_timepoints: n_timepoints of split (must be an even split)\n",
    "    \"\"\"\n",
    "    # Split the timeseries\n",
    "    split_ts = []\n",
    "    tmp = [split_timeseries(t,n_timepoints=n_timepoints) for t in timeseries]\n",
    "    for ts in tmp:\n",
    "        split_ts = split_ts + ts\n",
    "\n",
    "    #keep track of the corresponding labels\n",
    "    n = int(timeseries[0].shape[0]/n_timepoints)\n",
    "    split_labels = []\n",
    "    for l in labels:\n",
    "        split_labels.append(np.repeat(l,n))\n",
    "\n",
    "    #add a label for each split\n",
    "    split_labels.append(list(range(n))*len(timeseries))\n",
    "    return split_ts, split_labels\n",
    "\n",
    "def train_test_val_splits(split_ids,test_size=0.20,val_size=0.10,random_state=111):\n",
    "    \"\"\"Train test val split the data (in splits) so splits from a subject are in the same group.\n",
    "        returns INDEX for each split\n",
    "    \"\"\"\n",
    "    # Train test validation split of ids, then used to split dataframe\n",
    "    X = np.unique(split_ids)\n",
    "    y = list(range(len(X)))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size+val_size, random_state=random_state)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=val_size/(test_size+val_size), random_state=random_state)\n",
    "\n",
    "    train_idx = []\n",
    "    test_idx = []\n",
    "    val_idx = []\n",
    "    for i in range(len(split_ids)):\n",
    "        if split_ids[i] in X_train:\n",
    "            train_idx.append(i)\n",
    "        elif split_ids[i] in X_test:\n",
    "            test_idx.append(i)\n",
    "        elif split_ids[i]in X_val:\n",
    "            val_idx.append(i)\n",
    "\n",
    "    return train_idx,test_idx,val_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep data\n",
    " 1. Get cobre timeseries\n",
    " 2. Get cobre connectomes\n",
    " 3. Filter out subject with odd timeseries length\n",
    " 4. Get group average connectome\n",
    " 5. Build 8 k-NN graph from avg connectome\n",
    " 6. Split data: 70 training, 10 validation, 20 test\n",
    " 7. All data from same subject assigned to same Split\n",
    " 8. Cut time-series into bins of length time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cobreTimeWindows(Dataset):\n",
    "    def __init__(self,timeseries,connectomes,sub_ids,labels,test_size=0.20,val_size=0.10,random_state=111,n_timepoints=50,k=8):\n",
    "        \"\"\"\n",
    "        timeseries: list of arrays\n",
    "        connectomes: list of arrays\n",
    "        sub_ids: array of subject ids\n",
    "        labels: array of subject labels\n",
    "        \"\"\"\n",
    "        self.timeseries = timeseries\n",
    "        self.connectomes = connectomes\n",
    "        self.sub_ids = sub_ids\n",
    "        self.labels = labels\n",
    "\n",
    "        #make group connectomes\n",
    "        self.graph = make_group_graph(self.connectomes,k=k)\n",
    "\n",
    "        #split timeseries\n",
    "        self.split_timeseries,split_labs = split_ts_labels(self.timeseries,[self.sub_ids,self.labels],n_timepoints=n_timepoints)\n",
    "        self.split_sub_ids = split_labs[0]\n",
    "        self.split_labels = split_labs[1]\n",
    "        self.split_ids = split_labs[-1]\n",
    "\n",
    "        #train test val split the data (each sub's splits in one category only)\n",
    "        self.train_idx,self.test_idx,self.val_idx = train_test_val_splits(self.split_sub_ids,\n",
    "                                                                            test_size=test_size,\n",
    "                                                                            val_size=val_size,\n",
    "                                                                            random_state=random_state)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.split_sub_ids)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        ts = torch.from_numpy(self.split_timeseries[idx]).transpose(0,1)\n",
    "        sub_id = self.split_sub_ids[idx]\n",
    "        label = self.split_labels[idx]\n",
    "        split_id = self.split_ids[idx]\n",
    "        #return {'timeseries':ts,\n",
    "                 #\"sub_id\":sub_id, \n",
    "        #         'label':label, \n",
    "                 #\"split_id\":split_id\n",
    "        #         }\n",
    "        return ts,label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    " - C input channels (n time points of timeseries)\n",
    " - 6 GCN layers\n",
    " - 32 graph filters at each layer\n",
    " - Global average pooling layer\n",
    " - 2 fully connected layers\n",
    " - 256, 128 units\n",
    " - ReLU activation\n",
    " - Softmax last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self,edge_index,edge_weight,n_timepoints = 50):\n",
    "        super().__init__()\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_weight = edge_weight\n",
    "        self.conv1 = tg.nn.ChebConv(in_channels=n_timepoints,out_channels=32,K=2,bias=True)\n",
    "        self.conv2 = tg.nn.ChebConv(in_channels=32,out_channels=32,K=2,bias=True)\n",
    "        self.conv3 = tg.nn.ChebConv(in_channels=32,out_channels=32,K=2,bias=True)\n",
    "        self.conv4 = tg.nn.ChebConv(in_channels=32,out_channels=32,K=2,bias=True)\n",
    "        self.conv5 = tg.nn.ChebConv(in_channels=32,out_channels=32,K=2,bias=True)\n",
    "        self.conv6 = tg.nn.ChebConv(in_channels=32,out_channels=32,K=2,bias=True)\n",
    "        self.fc1 = nn.Linear(512*32, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x,self.edge_index,self.edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x,self.edge_index,self.edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x,self.edge_index,self.edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x,self.edge_index,self.edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x,self.edge_index,self.edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x,self.edge_index,self.edge_weight)\n",
    "        x = tg.nn.global_mean_pool(x,torch.from_numpy(np.array(range(x.size(0)),dtype=int)))\n",
    "        \n",
    "        x = x.view(-1, 512*32)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    " - Adam optimizer\n",
    " - Learning rate: 0.001\n",
    " - Weight decay: 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.sampler)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss, current = loss.item(), batch * len(X)\n",
    "        print(batch)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.sampler)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model.forward(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 0 is out of bounds for array of dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-163634616b8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create group graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_group_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create dataset (filters out subject with odd length timeseries)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-e3431c6600a1>\u001b[0m in \u001b[0;36mmake_group_graph\u001b[0;34m(connectomes, k)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_group_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnectomes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Group average connectome\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mavg_conn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnectomes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Undirected 8 k-NN graph as matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mrcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_count_reduce_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;31m# Make this warning show up first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_count_reduce_items\u001b[0;34m(arr, axis)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mitems\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_axis_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 0 is out of bounds for array of dimension 0"
     ]
    }
   ],
   "source": [
    "n_timepoints = 50\n",
    "batch_size = 128\n",
    "\n",
    "# Create group graph\n",
    "graph = make_group_graph(conn_path)\n",
    "\n",
    "# Create dataset (filters out subject with odd length timeseries)\n",
    "data = cobreTimeWindows(ts_path,pheno_path,n_timepoints=n_timepoints)\n",
    "\n",
    "# Create PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(data.train_idx)\n",
    "test_sampler = SubsetRandomSampler(data.test_idx)\n",
    "val_sampler = SubsetRandomSampler(data.val_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, sampler=test_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, sampler=val_sampler)\n",
    "\n",
    "# Create model\n",
    "gcn = GCN(graph.edge_index,graph.weight,n_timepoints=n_timepoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'connectomes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-42fa20800312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnectomes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'connectomes' is not defined"
     ]
    }
   ],
   "source": [
    "np.array(connectomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gcn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e1f0a8e5f2f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train and evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgcn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gcn' is not defined"
     ]
    }
   ],
   "source": [
    "# Train and evaluate model\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=0.001, weight_decay= 0.0005)\n",
    "\n",
    "epochs = 15\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, gcn, loss_fn, optimizer)\n",
    "    test_loop(test_loader, gcn, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
