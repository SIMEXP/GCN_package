{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('torch_geom': conda)",
   "metadata": {
    "interpreter": {
     "hash": "850987dbf3f390033835a5ad7e7a2ae79ec331baa410a33ee2d70de0ab51eda8"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch_geometric as tg\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import natsort"
   ]
  },
  {
   "source": [
    "# Prep data\n",
    " 1. Get cobre timeseries\n",
    " 2. Get cobre connectomes\n",
    " 3. Get group average connectome\n",
    " 4. Build 8 k-NN graph from avg connectome\n",
    " 5. Split data: 70 training, 10 validation, 20 test\n",
    " 6. All data from same subject assigned to same Split\n",
    " 7. Cut time-series into bins of length time window"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_path = '/home/harveyaa/Documents/fMRI/data/cobre/difumo/timeseries'\n",
    "conn_path = '/home/harveyaa/Documents/fMRI/data/cobre/difumo/connectomes'\n",
    "pheno_path = '/home/harveyaa/nilearn_data/cobre/phenotypic_data.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bad sub ID: 40075\n"
     ]
    }
   ],
   "source": [
    "timeseries = [np.load(os.path.join(ts_path,p)) for p in os.listdir(ts_path)]\n",
    "ids = [int(p.split('_')[1]) for p in os.listdir(ts_path)]\n",
    "\n",
    "# One subject has different length timeseries, ignore them for now\n",
    "not_150 = np.array([t.shape[0]!=150 for t in timeseries])\n",
    "print('Bad sub ID: {}'.format(np.array(ids)[not_150][0]))"
   ]
  },
  {
   "source": [
    "# Make Graph\n",
    "- Load connectomes\n",
    "- Get avg connectome\n",
    "- Get 8 knn graph from avg connectome"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_undirected(mat):\n",
    "    \"\"\"Takes an input adjacency matrix and makes it undirected (symmetric).\"\"\"\n",
    "    m = mat.copy()\n",
    "    mask = mat != mat.transpose()\n",
    "    vals = mat[mask] + mat.transpose()[mask]\n",
    "    m[mask] = vals\n",
    "    return m\n",
    "\n",
    "def knn_graph(mat,k=8,directed=False):\n",
    "    \"\"\"Takes an input matrix and returns a k-Nearest Neighbour weighted adjacency matrix.\"\"\"\n",
    "    m = mat.copy()\n",
    "    np.fill_diagonal(m,0)\n",
    "    slices = []\n",
    "    for i in range(m.shape[0]):\n",
    "        s = m[:,i]\n",
    "        not_neighbours = s.argsort()[:-k]\n",
    "        s[not_neighbours] = 0\n",
    "        slices.append(s)\n",
    "    if directed:\n",
    "        return np.array(slices)\n",
    "    else:\n",
    "        return make_undirected(np.array(slices))\n",
    "    \n",
    "def make_group_graph(conn_path):\n",
    "    # Load connectomes\n",
    "    connectomes = [np.load(os.path.join(conn_path,p)) for p in os.listdir(conn_path)]\n",
    "\n",
    "    # Group average connectome\n",
    "    avg_conn = np.array(connectomes).mean(axis=0)\n",
    "\n",
    "    # Undirected 8 k-NN graph as matrix\n",
    "    avg_conn8 = knn_graph(avg_conn,directed=False)\n",
    "\n",
    "    # Format matrix into graph for torch_geometric\n",
    "    graph = nx.convert_matrix.from_numpy_array(avg_conn8)\n",
    "    return tg.utils.from_networkx(graph)"
   ]
  },
  {
   "source": [
    "# Get train/test/validation data\n",
    "- Load timeseries and ids\n",
    "- Split timeseries of 150 volumes into time windows\n",
    "- Split data into train/test/validation\n",
    "  - All data from a given subject goes in the same bin"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_timeseries(ts,n_timepoints=50):\n",
    "    \"\"\"Takes an input timeseries and splits it into time windows of specified length. Need to choose a number that splits evenly.\"\"\"\n",
    "    if ts.shape[0] % n_timepoints != 0:\n",
    "        raise ValueError('Yikes choose a divisor for now')\n",
    "    else:\n",
    "        n_splits = ts.shape[0] / n_timepoints\n",
    "        return np.split(ts,n_splits)\n",
    "\n",
    "def split_ts_labels(timeseries,labels,n_timepoints=50):\n",
    "    \"\"\"\n",
    "    timeseries: list of timeseries\n",
    "    labels: list of lists (of accompanying labels)\n",
    "    n_timepoints: n_timepoints of split (must be an even split)\n",
    "    \"\"\"\n",
    "    # Split the timeseries\n",
    "    split_ts = []\n",
    "    for ts in map(split_timeseries,timeseries):\n",
    "        split_ts = split_ts + ts\n",
    "\n",
    "    #keep track of the corresponding labels\n",
    "    n = int(timeseries[0].shape[0]/n_timepoints)\n",
    "    split_labels = []\n",
    "    for l in labels:\n",
    "        split_labels.append(np.repeat(l,n))\n",
    "\n",
    "    #add a label for each split\n",
    "    split_labels.append(list(range(n))*len(timeseries))\n",
    "    return split_ts, split_labels\n",
    "\n",
    "def train_test_val_splits(split_ids,test_size=0.20,val_size=0.10,random_state=111):\n",
    "    \"\"\"Train test val split the data (in splits) so splits from a subject are in the same group.\n",
    "        returns INDEX for each split\n",
    "    \"\"\"\n",
    "    # Train test validation split of ids, then used to split dataframe\n",
    "    X = np.unique(split_ids)\n",
    "    y = list(range(len(X)))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size+val_size, random_state=random_state)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=val_size/(test_size+val_size), random_state=random_state)\n",
    "\n",
    "    train_idx = []\n",
    "    test_idx = []\n",
    "    val_idx = []\n",
    "    for i in range(len(split_ids)):\n",
    "        if split_ids[i] in X_train:\n",
    "            train_idx.append(i)\n",
    "        elif split_ids[i] in X_test:\n",
    "            test_idx.append(i)\n",
    "        elif split_ids[i]in X_val:\n",
    "            val_idx.append(i)\n",
    "\n",
    "    return train_idx,test_idx,val_idx\n",
    "    \n",
    "class cobreTimeWindows(Dataset):\n",
    "    def __init__(self,ts_path,pheno_path,test_size=0.20,val_size=0.10,random_state=111,n_timepoints=50):\n",
    "        self.pheno_path = pheno_path\n",
    "        pheno = pd.read_csv(pheno_path,delimiter='\\t')\n",
    "        pheno = pheno[pheno['ID']!=40075]\n",
    "        pheno.sort_values('ID',inplace=True)\n",
    "        self.labels = pheno['Subject Type'].map({'Patient':1,'Control':0}).tolist()\n",
    "\n",
    "        self.ts_path = ts_path\n",
    "        self.timeseries = [np.load(os.path.join(ts_path,p)) for p in natsort.natsorted(os.listdir(ts_path))]\n",
    "        self.sub_ids = [int(p.split('_')[1]) for p in natsort.natsorted(os.listdir(ts_path))]\n",
    "\n",
    "        #filter out bad sub\n",
    "        idx = self.sub_ids.index(40075)\n",
    "        del self.sub_ids[idx]\n",
    "        del self.timeseries[idx]\n",
    "\n",
    "        #split timeseries\n",
    "        self.split_timeseries,split_labs = split_ts_labels(self.timeseries,[self.sub_ids,self.labels],n_timepoints=n_timepoints)\n",
    "        self.split_sub_ids = split_labs[0]\n",
    "        self.split_labels = split_labs[1]\n",
    "        self.split_ids = split_labs[-1]\n",
    "\n",
    "        #train test val split the data (each sub's splits in one category only)\n",
    "        self.train_idx,self.test_idx,self.val_idx = train_test_val_splits(self.split_sub_ids,\n",
    "                                                                            test_size=test_size,\n",
    "                                                                            val_size=val_size,\n",
    "                                                                            random_state=random_state)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.split_sub_ids)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        ts = torch.from_numpy(self.split_timeseries[idx]).transpose(0,1)\n",
    "        sub_id = self.split_sub_ids[idx]\n",
    "        label = self.split_labels[idx]\n",
    "        split_id = self.split_ids[idx]\n",
    "        #return {'timeseries':ts,\n",
    "                 #\"sub_id\":sub_id, \n",
    "        #         'label':label, \n",
    "                 #\"split_id\":split_id\n",
    "        #         }\n",
    "        return ts,label"
   ]
  },
  {
   "source": [
    "# Model\n",
    " - C input channels (n time points of timeseries)\n",
    " - 6 GCN layers\n",
    " - 32 graph filters at each layer\n",
    " - Global average pooling layer\n",
    " - 2 fully connected layers\n",
    " - 256, 128 units\n",
    " - ReLU activation\n",
    " - Softmax last layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self,edge_index,edge_weight,n_timepoints = 50):\n",
    "        super().__init__()\n",
    "        #forward(x, edge_index, edge_weight: Optional[torch.Tensor] = None\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_weight = edge_weight\n",
    "        self.conv1 = tg.nn.ChebConv(in_channels=n_timepoints,out_channels=32,K=2,bias=True)\n",
    "        self.conv2 = tg.nn.ChebConv(in_channels=32,out_channels=32,K=2,bias=True)\n",
    "        self.conv3 = tg.nn.ChebConv(in_channels=32,out_channels=32,K=2,bias=True)\n",
    "        self.conv4 = tg.nn.ChebConv(in_channels=32,out_channels=32,K=2,bias=True)\n",
    "        self.conv5 = tg.nn.ChebConv(in_channels=32,out_channels=32,K=2,bias=True)\n",
    "        self.conv6 = tg.nn.ChebConv(in_channels=32,out_channels=32,K=2,bias=True)\n",
    "        #self.fc1 = nn.Linear(512, 256)\n",
    "        #self.fc2 = nn.Linear(256, 128)\n",
    "        #self.fc3 = nn.Linear(128,2)\n",
    "        self.fc1 = nn.Linear(512*32, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #print(x.size())\n",
    "        x = self.conv1(x,self.edge_index,self.edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x,self.edge_index,self.edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x,self.edge_index,self.edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x,self.edge_index,self.edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x,self.edge_index,self.edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x,self.edge_index,self.edge_weight)\n",
    "        #print(x.size())\n",
    "        x = tg.nn.global_mean_pool(x,torch.from_numpy(np.array(range(x.size(0)),dtype=int)))\n",
    "\n",
    "        #print(x.size())\n",
    "        ####x = torch.transpose(x,1,2)\n",
    "        x = x.view(-1, 512*32)\n",
    "        x = self.fc1(x)\n",
    "        #print(x.size())\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        #print(x.size())\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        #print(x.size())\n",
    "        #x = F.softmax(x,dim=0)\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "np.array(range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.sampler)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        #print(pred.size())\n",
    "        #print(y)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss, current = loss.item(), batch * len(X)\n",
    "        print(batch)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.sampler)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model.forward(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = make_group_graph(conn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cobreTimeWindows(ts_path,pheno_path,n_timepoints=15)\n",
    "batch_size = 128\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(data.train_idx)\n",
    "test_sampler = SubsetRandomSampler(data.test_idx)\n",
    "val_sampler = SubsetRandomSampler(data.val_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, sampler=test_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn = GCN(graph.edge_index,graph.weight,n_timepoints=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([512, 50])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "data[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "0\n",
      "loss: 0.716936  [    0/  303]\n",
      "1\n",
      "loss: 13.215805  [  128/  303]\n",
      "2\n",
      "loss: 82.561584  [   94/  303]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 38438.609195 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "0\n",
      "loss: 3978590.500000  [    0/  303]\n",
      "1\n",
      "loss:     inf  [  128/  303]\n",
      "2\n",
      "loss:     nan  [   94/  303]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss:      nan \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "0\n",
      "loss:     nan  [    0/  303]\n",
      "1\n",
      "loss:     nan  [  128/  303]\n",
      "2\n",
      "loss:     nan  [   94/  303]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss:      nan \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "0\n",
      "loss:     nan  [    0/  303]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(gcn.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, gcn, loss_fn, optimizer)\n",
    "    test_loop(test_loader, gcn, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): ChebConv(50, 32, K=2, normalization=sym)\n",
       "  (conv2): ChebConv(32, 32, K=2, normalization=sym)\n",
       "  (conv3): ChebConv(32, 32, K=2, normalization=sym)\n",
       "  (conv4): ChebConv(32, 32, K=2, normalization=sym)\n",
       "  (conv5): ChebConv(32, 32, K=2, normalization=sym)\n",
       "  (conv6): ChebConv(32, 32, K=2, normalization=sym)\n",
       "  (fc1): Linear(in_features=16384, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "gcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
       "       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
       "       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
       "       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
       "       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
       "       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "       494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
       "       507, 508, 509, 510, 511])"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "np.unique(graph.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}