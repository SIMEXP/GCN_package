{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('torch_geom': conda)",
   "metadata": {
    "interpreter": {
     "hash": "850987dbf3f390033835a5ad7e7a2ae79ec331baa410a33ee2d70de0ab51eda8"
    }
   }
  },
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch_geometric as tg\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "source": [
    "# Define paths\n",
    "Load timeseries and find id of subject with odd length timeseries."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path = os.path.join(\"..\", \"data\", \"cobre_difumo512\")\n",
    "data_path = '/home/harveyaa/Documents/fMRI/data/cobre'\n",
    "ts_path = os.path.join(data_path, \"difumo\", \"timeseries\")\n",
    "conn_path = os.path.join(data_path, \"difumo\", \"connectomes\")\n",
    "pheno_path = os.path.join(data_path, \"difumo\", \"phenotypic_data.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bad sub ID: 40075\n"
     ]
    }
   ],
   "source": [
    "timeseries = [np.load(os.path.join(ts_path, p)) for p in os.listdir(ts_path)]\n",
    "ids = [int(p.split('_')[1]) for p in os.listdir(ts_path)]\n",
    "\n",
    "# One subject has different length timeseries, ignore them for now\n",
    "not_150 = np.array([t.shape[0]!=150 for t in timeseries])\n",
    "print('Bad sub ID: {}'.format(np.array(ids)[not_150][0]))"
   ]
  },
  {
   "source": [
    "# Make Graph\n",
    "- Load connectomes\n",
    "- Get avg connectome\n",
    "- Get 8 knn graph from avg connectome"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_undirected(mat):\n",
    "    \"\"\"Takes an input adjacency matrix and makes it undirected (symmetric).\"\"\"\n",
    "    m = mat.copy()\n",
    "    mask = mat != mat.transpose()\n",
    "    vals = mat[mask] + mat.transpose()[mask]\n",
    "    m[mask] = vals\n",
    "    return m\n",
    "\n",
    "def knn_graph(mat,k=8):\n",
    "    \"\"\"Takes an input matrix and returns a k-Nearest Neighbour weighted adjacency matrix.\"\"\"\n",
    "    is_undirected = (mat == mat.T).all()\n",
    "    m = np.abs(mat.copy())\n",
    "    np.fill_diagonal(m,0)\n",
    "    slices = []\n",
    "    for i in range(m.shape[0]):\n",
    "        s = m[:,i]\n",
    "        not_neighbours = s.argsort()[:-k]\n",
    "        s[not_neighbours] = 0\n",
    "        slices.append(s)\n",
    "    if is_undirected:\n",
    "        return np.array(slices)\n",
    "    else:\n",
    "        return make_undirected(np.array(slices))\n",
    "    \n",
    "def make_group_graph(conn_path,k=8):\n",
    "    # Load connectomes\n",
    "    connectomes = [np.load(os.path.join(conn_path,p)) for p in os.listdir(conn_path)]\n",
    "\n",
    "    # Group average connectome\n",
    "    avg_conn = np.array(connectomes).mean(axis=0)\n",
    "\n",
    "    # Undirected 8 k-NN graph as matrix\n",
    "    avg_conn8 = knn_graph(avg_conn,k=k)\n",
    "\n",
    "    # Format matrix into graph for torch_geometric\n",
    "    graph = nx.convert_matrix.from_numpy_array(avg_conn8)\n",
    "    return tg.utils.from_networkx(graph)"
   ]
  },
  {
   "source": [
    "# Get train/test/validation data\n",
    "- Load timeseries and ids\n",
    "- Split timeseries of 150 volumes into time windows\n",
    "- Split data into train/test/validation\n",
    "  - All data from a given subject goes in the same bin"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_timeseries(ts,n_timepoints=50):\n",
    "    \"\"\"Takes an input timeseries and splits it into time windows of specified length. Need to choose a number that splits evenly.\"\"\"\n",
    "    if ts.shape[0] % n_timepoints != 0:\n",
    "        raise ValueError('Yikes choose a divisor for now')\n",
    "    else:\n",
    "        n_splits = ts.shape[0] / n_timepoints\n",
    "        return np.split(ts,n_splits)\n",
    "\n",
    "def split_ts_labels(timeseries,labels,n_timepoints=50):\n",
    "    \"\"\"\n",
    "    timeseries: list of timeseries\n",
    "    labels: list of lists (of accompanying labels)\n",
    "    n_timepoints: n_timepoints of split (must be an even split)\n",
    "    \"\"\"\n",
    "    # Split the timeseries\n",
    "    split_ts = []\n",
    "    tmp = [split_timeseries(t,n_timepoints=n_timepoints) for t in timeseries]\n",
    "    for ts in tmp:\n",
    "        split_ts = split_ts + ts\n",
    "\n",
    "    #keep track of the corresponding labels\n",
    "    n = int(timeseries[0].shape[0]/n_timepoints)\n",
    "    split_labels = []\n",
    "    for l in labels:\n",
    "        split_labels.append(np.repeat(l,n))\n",
    "\n",
    "    #add a label for each split\n",
    "    split_labels.append(list(range(n))*len(timeseries))\n",
    "    return split_ts, split_labels\n",
    "\n",
    "def train_test_val_splits(split_ids,test_size=0.20,val_size=0.10,random_state=111):\n",
    "    \"\"\"Train test val split the data (in splits) so splits from a subject are in the same group.\n",
    "        returns INDEX for each split\n",
    "    \"\"\"\n",
    "    # Train test validation split of ids, then used to split dataframe\n",
    "    X = np.unique(split_ids)\n",
    "    y = list(range(len(X)))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size+val_size, random_state=random_state)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=val_size/(test_size+val_size), random_state=random_state)\n",
    "\n",
    "    train_idx = []\n",
    "    test_idx = []\n",
    "    val_idx = []\n",
    "    for i in range(len(split_ids)):\n",
    "        if split_ids[i] in X_train:\n",
    "            train_idx.append(i)\n",
    "        elif split_ids[i] in X_test:\n",
    "            test_idx.append(i)\n",
    "        elif split_ids[i]in X_val:\n",
    "            val_idx.append(i)\n",
    "\n",
    "    return train_idx,test_idx,val_idx"
   ]
  },
  {
   "source": [
    "# Prep data\n",
    " 1. Get cobre timeseries\n",
    " 2. Get cobre connectomes\n",
    " 3. Filter out subject with odd timeseries length\n",
    " 4. Get group average connectome\n",
    " 5. Build 8 k-NN graph from avg connectome\n",
    " 6. Split data: 70 training, 10 validation, 20 test\n",
    " 7. All data from same subject assigned to same Split\n",
    " 8. Cut time-series into bins of length time window"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cobreTimeWindows(Dataset):\n",
    "    def __init__(self,ts_path,pheno_path,test_size=0.20,val_size=0.10,random_state=111,n_timepoints=50):\n",
    "        self.pheno_path = pheno_path\n",
    "        pheno = pd.read_csv(pheno_path,delimiter='\\t')\n",
    "        pheno = pheno[pheno['ID']!=40075]\n",
    "        pheno.sort_values('ID',inplace=True)\n",
    "        self.labels = pheno['Subject Type'].map({'Patient':1,'Control':0}).tolist()\n",
    "\n",
    "        self.ts_path = ts_path\n",
    "        self.timeseries = [np.load(os.path.join(ts_path,p)) for p in sorted(os.listdir(ts_path))]\n",
    "        self.sub_ids = [int(p.split('_')[1]) for p in sorted(os.listdir(ts_path))]\n",
    "\n",
    "        #filter out bad sub\n",
    "        idx = self.sub_ids.index(40075)\n",
    "        del self.sub_ids[idx]\n",
    "        del self.timeseries[idx]\n",
    "\n",
    "        #split timeseries\n",
    "        self.split_timeseries,split_labs = split_ts_labels(self.timeseries,[self.sub_ids,self.labels],n_timepoints=n_timepoints)\n",
    "        self.split_sub_ids = split_labs[0]\n",
    "        self.split_labels = split_labs[1]\n",
    "        self.split_ids = split_labs[-1]\n",
    "\n",
    "        #train test val split the data (each sub's splits in one category only)\n",
    "        self.train_idx,self.test_idx,self.val_idx = train_test_val_splits(self.split_sub_ids,\n",
    "                                                                            test_size=test_size,\n",
    "                                                                            val_size=val_size,\n",
    "                                                                            random_state=random_state)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.split_sub_ids)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        ts = torch.from_numpy(self.split_timeseries[idx]).transpose(0,1)\n",
    "        sub_id = self.split_sub_ids[idx]\n",
    "        label = self.split_labels[idx]\n",
    "        split_id = self.split_ids[idx]\n",
    "        #return {'timeseries':ts,\n",
    "                 #\"sub_id\":sub_id, \n",
    "        #         'label':label, \n",
    "                 #\"split_id\":split_id\n",
    "        #         }\n",
    "        return ts,label"
   ]
  },
  {
   "source": [
    "# Model\n",
    " - C input channels (n time points of timeseries)\n",
    " - 6 GCN layers\n",
    " - 32 graph filters at each layer\n",
    " - Global average pooling layer\n",
    " - 2 fully connected layers\n",
    " - 256, 128 units\n",
    " - ReLU activation\n",
    " - Softmax last layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self,edge_index,edge_weight,n_timepoints = 50):\n",
    "        super().__init__()\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_weight = edge_weight\n",
    "        self.conv1 = tg.nn.ChebConv(in_channels=n_timepoints,out_channels=32,K=2,bias=True)\n",
    "        self.conv2 = tg.nn.ChebConv(in_channels=32,out_channels=32,K=2,bias=True)\n",
    "        self.conv3 = tg.nn.ChebConv(in_channels=32,out_channels=32,K=2,bias=True)\n",
    "        self.conv4 = tg.nn.ChebConv(in_channels=32,out_channels=32,K=2,bias=True)\n",
    "        self.conv5 = tg.nn.ChebConv(in_channels=32,out_channels=32,K=2,bias=True)\n",
    "        self.conv6 = tg.nn.ChebConv(in_channels=32,out_channels=32,K=2,bias=True)\n",
    "        self.fc1 = nn.Linear(512*32, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x,self.edge_index,self.edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x,self.edge_index,self.edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x,self.edge_index,self.edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x,self.edge_index,self.edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x,self.edge_index,self.edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x,self.edge_index,self.edge_weight)\n",
    "        x = tg.nn.global_mean_pool(x,torch.from_numpy(np.array(range(x.size(0)),dtype=int)))\n",
    "        \n",
    "        x = x.view(-1, 512*32)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "source": [
    "# Model Training\n",
    " - Adam optimizer\n",
    " - Learning rate: 0.001\n",
    " - Weight decay: 0.0005"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.sampler)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss, current = loss.item(), batch * len(X)\n",
    "        print(batch)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.sampler)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model.forward(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timepoints = 50\n",
    "batch_size = 128\n",
    "\n",
    "# Create group graph\n",
    "graph = make_group_graph(conn_path)\n",
    "\n",
    "# Create dataset (filters out subject with odd length timeseries)\n",
    "data = cobreTimeWindows(ts_path,pheno_path,n_timepoints=n_timepoints)\n",
    "\n",
    "# Create PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(data.train_idx)\n",
    "test_sampler = SubsetRandomSampler(data.test_idx)\n",
    "val_sampler = SubsetRandomSampler(data.val_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, sampler=test_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, sampler=val_sampler)\n",
    "\n",
    "# Create model\n",
    "gcn = GCN(graph.edge_index,graph.weight,n_timepoints=n_timepoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "0\n",
      "loss: 0.610475  [    0/  303]\n",
      "1\n",
      "loss: 1.070779  [  128/  303]\n",
      "2\n",
      "loss: 1.479934  [   94/  303]\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 0.011561 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "0\n",
      "loss: 0.979142  [    0/  303]\n",
      "1\n",
      "loss: 0.757028  [  128/  303]\n",
      "2\n",
      "loss: 1.033664  [   94/  303]\n",
      "Test Error: \n",
      " Accuracy: 47.1%, Avg loss: 0.010839 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "0\n",
      "loss: 0.709783  [    0/  303]\n",
      "1\n",
      "loss: 0.580474  [  128/  303]\n",
      "2\n",
      "loss: 0.718773  [   94/  303]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 0.008650 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "0\n",
      "loss: 0.688126  [    0/  303]\n",
      "1\n",
      "loss: 0.684290  [  128/  303]\n",
      "2\n",
      "loss: 0.614238  [   94/  303]\n",
      "Test Error: \n",
      " Accuracy: 47.1%, Avg loss: 0.009737 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "0\n",
      "loss: 0.626292  [    0/  303]\n",
      "1\n",
      "loss: 0.663621  [  128/  303]\n",
      "2\n",
      "loss: 0.594111  [   94/  303]\n",
      "Test Error: \n",
      " Accuracy: 47.1%, Avg loss: 0.009151 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "0\n",
      "loss: 0.549400  [    0/  303]\n",
      "1\n",
      "loss: 0.636889  [  128/  303]\n",
      "2\n",
      "loss: 0.516136  [   94/  303]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 0.008098 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "0\n",
      "loss: 0.606095  [    0/  303]\n",
      "1\n",
      "loss: 0.490908  [  128/  303]\n",
      "2\n",
      "loss: 0.357158  [   94/  303]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.008693 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "0\n",
      "loss: 0.437655  [    0/  303]\n",
      "1\n",
      "loss: 0.386010  [  128/  303]\n",
      "2\n",
      "loss: 0.415835  [   94/  303]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 0.009210 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "0\n",
      "loss: 0.393582  [    0/  303]\n",
      "1\n",
      "loss: 0.372211  [  128/  303]\n",
      "2\n",
      "loss: 0.307739  [   94/  303]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 0.008799 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "0\n",
      "loss: 0.285389  [    0/  303]\n",
      "1\n",
      "loss: 0.308738  [  128/  303]\n",
      "2\n",
      "loss: 0.384006  [   94/  303]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.010267 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "0\n",
      "loss: 0.223157  [    0/  303]\n",
      "1\n",
      "loss: 0.241399  [  128/  303]\n",
      "2\n",
      "loss: 0.213332  [   94/  303]\n",
      "Test Error: \n",
      " Accuracy: 49.4%, Avg loss: 0.011218 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "0\n",
      "loss: 0.232844  [    0/  303]\n",
      "1\n",
      "loss: 0.151701  [  128/  303]\n",
      "2\n",
      "loss: 0.091962  [   94/  303]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 0.012244 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "0\n",
      "loss: 0.130359  [    0/  303]\n",
      "1\n",
      "loss: 0.120067  [  128/  303]\n",
      "2\n",
      "loss: 0.093597  [   94/  303]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 0.012506 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "0\n",
      "loss: 0.072498  [    0/  303]\n",
      "1\n",
      "loss: 0.063975  [  128/  303]\n",
      "2\n",
      "loss: 0.048500  [   94/  303]\n",
      "Test Error: \n",
      " Accuracy: 46.0%, Avg loss: 0.016177 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "0\n",
      "loss: 0.052763  [    0/  303]\n",
      "1\n",
      "loss: 0.044431  [  128/  303]\n",
      "2\n",
      "loss: 0.039851  [   94/  303]\n",
      "Test Error: \n",
      " Accuracy: 49.4%, Avg loss: 0.016619 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate model\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=0.001, weight_decay= 0.0005)\n",
    "\n",
    "epochs = 15\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, gcn, loss_fn, optimizer)\n",
    "    test_loop(test_loader, gcn, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}